
"""
æ¶æ„å¸ˆçš„æ ¸å¿ƒå·¥ä½œï¼šä¸æ˜¯é€‰æ‹©æ¡†æ¶ï¼Œè€Œæ˜¯è®¾è®¡ä¸€ä¸ªè®©æ¡†æ¶é€‰æ‹©å¯é€†çš„æ¶æ„
"""

from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import json
from datetime import datetime

# ==================== 1. æŠ½è±¡å±‚å®šä¹‰ ====================
# è¿™éƒ¨åˆ†ä»£ç ä¸€æ—¦ç¡®å®šï¼Œæœªæ¥3å¹´éƒ½ä¸åº”è¯¥æ”¹åŠ¨
# å®ƒå®šä¹‰äº†ç³»ç»Ÿ"åšä»€ä¹ˆ"ï¼Œè€Œä¸æ˜¯"æ€ä¹ˆåš"

class Document(ABC):
    """æ–‡æ¡£æŠ½è±¡ï¼Œä¸ä¾èµ–ä»»ä½•å…·ä½“æ¡†æ¶"""
    @property
    @abstractmethod
    def content(self) -> str: ...
    
    @property
    @abstractmethod
    def metadata(self) -> Dict[str, Any]: ...


class VectorStore(ABC):
    """å‘é‡å­˜å‚¨æŠ½è±¡"""
    @abstractmethod
    def search(self, query: str, top_k: int = 5) -> List[Document]: ...
    
    @abstractmethod
    def add_documents(self, documents: List[Document]): ...
    
    @classmethod
    @abstractmethod
    def create_from_documents(cls, documents: List[Document], **kwargs) -> 'VectorStore': ...


class RAGGenerator(ABC):
    """RAGç”Ÿæˆå™¨æŠ½è±¡"""
    @abstractmethod
    def generate(self, query: str, context: List[Document]) -> str: ...


class RAGSystem(ABC):
    """å®Œæ•´çš„RAGç³»ç»ŸæŠ½è±¡"""
    @abstractmethod
    def query(self, query: str) -> Dict[str, Any]: ...
    
    @abstractmethod
    def ingest(self, documents: List[Document]): ...


# ==================== 2. é€‚é…å™¨å±‚ ====================
# è¿™éƒ¨åˆ†ä»£ç å¯ä»¥æ›¿æ¢ï¼Œæ˜¯æ¡†æ¶é€‰æ‹©çš„å…·ä½“å®ç°
# æ¯ä¸ªé€‚é…å™¨éƒ½æ˜¯ç‹¬ç«‹çš„ï¼Œå¯æ’æ‹”

# ---------- LlamaIndex é€‚é…å™¨ ----------
class LlamaIndexDocument(Document):
    """LlamaIndexçš„æ–‡æ¡£é€‚é…å™¨"""
    def __init__(self, llama_document):
        self._doc = llama_document
    
    @property
    def content(self) -> str:
        return self._doc.text
    
    @property
    def metadata(self) -> Dict[str, Any]:
        return self._doc.metadata or {}


class LlamaIndexVectorStore(VectorStore):
    """LlamaIndexå‘é‡å­˜å‚¨é€‚é…å™¨"""
    
    def __init__(self, index):
        # è¿™é‡Œéšè—äº†LlamaIndexçš„å…·ä½“å®ç°
        self._index = index
        self._retriever = index.as_retriever(similarity_top_k=5)
    
    def search(self, query: str, top_k: int = 5) -> List[Document]:
        # é€‚é…å™¨æ¨¡å¼ï¼šå°†æ¡†æ¶æ¥å£è½¬æ¢ä¸ºæˆ‘ä»¬çš„æŠ½è±¡æ¥å£
        nodes = self._retriever.retrieve(query)
        return [LlamaIndexDocument(node) for node in nodes[:top_k]]
    
    def add_documents(self, documents: List[Document]):
        # å¦‚æœLlamaIndexä¸æ”¯æŒåŠ¨æ€æ·»åŠ ï¼Œè¿™é‡Œä¼šæŠ›å‡ºæ˜ç¡®å¼‚å¸¸
        raise NotImplementedError("LlamaIndexä¸æ”¯æŒåŠ¨æ€æ·»åŠ æ–‡æ¡£")
    
    @classmethod
    def create_from_documents(cls, documents: List[Document], **kwargs):
        # ä»æ–‡æ¡£åˆ›å»ºç´¢å¼•
        from llama_index.core import VectorStoreIndex
        from llama_index.core.schema import TextNode
        
        # å°†æˆ‘ä»¬çš„Documentè½¬æ¢ä¸ºLlamaIndexçš„TextNode
        nodes = []
        for doc in documents:
            if isinstance(doc, LlamaIndexDocument):
                nodes.append(doc._doc)
            else:
                # åˆ›å»ºé€‚é…èŠ‚ç‚¹
                node = TextNode(
                    text=doc.content,
                    metadata=doc.metadata
                )
                nodes.append(node)
        
        index = VectorStoreIndex(nodes, **kwargs)
        return cls(index)


# ---------- LangChain é€‚é…å™¨ ----------
class LangChainDocument(Document):
    """LangChainçš„æ–‡æ¡£é€‚é…å™¨"""
    def __init__(self, lc_document):
        self._doc = lc_document
    
    @property
    def content(self) -> str:
        return self._doc.page_content
    
    @property
    def metadata(self) -> Dict[str, Any]:
        return self._doc.metadata


class LangChainVectorStore(VectorStore):
    """LangChainå‘é‡å­˜å‚¨é€‚é…å™¨"""
    
    def __init__(self, vectorstore, retriever):
        self._vectorstore = vectorstore
        self._retriever = retriever
    
    def search(self, query: str, top_k: int = 5) -> List[Document]:
        docs = self._retriever.get_relevant_documents(query)[:top_k]
        return [LangChainDocument(doc) for doc in docs]
    
    def add_documents(self, documents: List[Document]):
        # LangChainé€šå¸¸æ”¯æŒåŠ¨æ€æ·»åŠ 
        lc_docs = []
        for doc in documents:
            if isinstance(doc, LangChainDocument):
                lc_docs.append(doc._doc)
            else:
                from langchain_core.documents import Document as LCDocument
                lc_docs.append(LCDocument(
                    page_content=doc.content,
                    metadata=doc.metadata
                ))
        self._vectorstore.add_documents(lc_docs)
    
    @classmethod
    def create_from_documents(cls, documents: List[Document], **kwargs):
        from langchain_openai import OpenAIEmbeddings
        from langchain_chroma import Chroma
        from langchain_core.documents import Document as LCDocument
        
        # è½¬æ¢ä¸ºLangChainæ–‡æ¡£
        lc_docs = []
        for doc in documents:
            if isinstance(doc, LangChainDocument):
                lc_docs.append(doc._doc)
            else:
                lc_docs.append(LCDocument(
                    page_content=doc.content,
                    metadata=doc.metadata
                ))
        
        # åˆ›å»ºå‘é‡å­˜å‚¨
        embeddings = OpenAIEmbeddings()
        vectorstore = Chroma.from_documents(
            documents=lc_docs,
            embedding=embeddings,
            **kwargs
        )
        
        retriever = vectorstore.as_retriever(
            search_kwargs={"k": 5}
        )
        
        return cls(vectorstore, retriever)


# ==================== 3. é…ç½®ä¸å·¥å‚ ====================
# è¿™éƒ¨åˆ†å†³å®šä½¿ç”¨å“ªä¸ªæ¡†æ¶ï¼Œä½†å¯ä»¥éšæ—¶åˆ‡æ¢

@dataclass
class RAGConfig:
    """é…ç½®ç±»ï¼šæ‰€æœ‰å¯åˆ‡æ¢çš„å†³ç­–ç‚¹éƒ½åœ¨è¿™é‡Œ"""
    framework: str = "llamaindex"  # å¯åˆ‡æ¢ï¼šllamaindex, langchain, haystack
    embedding_model: str = "text-embedding-3-small"
    llm_model: str = "gpt-4-turbo"
    vector_store_type: str = "chroma"
    chunk_size: int = 512
    chunk_overlap: int = 50
    
    # åˆ‡æ¢æˆæœ¬è®°å½•
    switch_cost_estimate: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.switch_cost_estimate is None:
            self.switch_cost_estimate = {
                "llamaindex_to_langchain": {
                    "code_changes": "é€‚é…å™¨å±‚é‡å†™",
                    "estimated_hours": 8,
                    "data_migration": "æ— éœ€è¿ç§»",
                    "risk_level": "ä½"
                },
                "langchain_to_llamaindex": {
                    "code_changes": "é€‚é…å™¨å±‚é‡å†™",
                    "estimated_hours": 8,
                    "data_migration": "å¯èƒ½éœ€è¦é‡å»ºç´¢å¼•",
                    "risk_level": "ä¸­"
                }
            }


class RAGFactory:
    """å·¥å‚ç±»ï¼šæ ¹æ®é…ç½®åˆ›å»ºå…·ä½“ç»„ä»¶"""
    
    @staticmethod
    def create_vector_store(documents: List[Document], config: RAGConfig) -> VectorStore:
        """åˆ›å»ºå‘é‡å­˜å‚¨ï¼Œå…·ä½“å®ç°ç”±é…ç½®å†³å®š"""
        if config.framework == "llamaindex":
            return LlamaIndexVectorStore.create_from_documents(documents)
        elif config.framework == "langchain":
            return LangChainVectorStore.create_from_documents(documents)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¡†æ¶: {config.framework}")
    
    @staticmethod
    def create_generator(config: RAGConfig) -> RAGGenerator:
        """åˆ›å»ºç”Ÿæˆå™¨ï¼Œå…·ä½“å®ç°ç”±é…ç½®å†³å®š"""
        if config.framework == "llamaindex":
            from .llamaindex_generator import LlamaIndexGenerator
            return LlamaIndexGenerator(config.llm_model)
        elif config.framework == "langchain":
            from .langchain_generator import LangChainGenerator
            return LangChainGenerator(config.llm_model)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¡†æ¶: {config.framework}")


# ==================== 4. å…·ä½“çš„RAGç³»ç»Ÿå®ç° ====================

class SimpleRAGSystem(RAGSystem):
    """å…·ä½“çš„RAGç³»ç»Ÿå®ç°ï¼Œä½†é€šè¿‡æŠ½è±¡å±‚éš”ç¦»æ¡†æ¶ä¾èµ–"""
    
    def __init__(self, config: RAGConfig):
        self.config = config
        self.vector_store: Optional[VectorStore] = None
        self.generator: Optional[RAGGenerator] = None
        self.query_history = []
        
        # ç›‘æ§æŒ‡æ ‡
        self.metrics = {
            "total_queries": 0,
            "avg_response_time": 0.0,
            "framework_errors": 0,
            "last_framework_check": datetime.now()
        }
    
    def ingest(self, documents: List[Document]):
        """æ‘„å–æ–‡æ¡£"""
        self.vector_store = RAGFactory.create_vector_store(documents, self.config)
        self.generator = RAGFactory.create_generator(self.config)
    
    def query(self, query: str) -> Dict[str, Any]:
        """æŸ¥è¯¢"""
        start_time = datetime.now()
        
        try:
            # 1. æ£€ç´¢
            context_docs = self.vector_store.search(query)
            
            # 2. ç”Ÿæˆ
            answer = self.generator.generate(query, context_docs)
            
            # 3. è®°å½•ç»“æœ
            response_time = (datetime.now() - start_time).total_seconds()
            self._record_query(query, answer, response_time, True)
            
            return {
                "answer": answer,
                "sources": [doc.metadata for doc in context_docs],
                "response_time": response_time,
                "framework": self.config.framework,
                "success": True
            }
            
        except Exception as e:
            # è®°å½•æ¡†æ¶é”™è¯¯
            self.metrics["framework_errors"] += 1
            self._record_query(query, str(e), 0, False)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è§¦å‘æ¡†æ¶åˆ‡æ¢
            if self._should_switch_framework():
                return self._try_fallback_framework(query)
            
            raise
    
    def _record_query(self, query: str, answer: str, response_time: float, success: bool):
        """è®°å½•æŸ¥è¯¢å†å²ï¼Œç”¨äºç›‘æ§å’Œå†³ç­–"""
        self.query_history.append({
            "timestamp": datetime.now(),
            "query": query,
            "answer": answer,
            "response_time": response_time,
            "success": success,
            "framework": self.config.framework
        })
        
        # æ›´æ–°æŒ‡æ ‡
        self.metrics["total_queries"] += 1
        if success:
            total_time = self.metrics["avg_response_time"] * (self.metrics["total_queries"] - 1)
            self.metrics["avg_response_time"] = (total_time + response_time) / self.metrics["total_queries"]
    
    def _should_switch_framework(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ‡æ¢æ¡†æ¶çš„å†³ç­–é€»è¾‘"""
        # åŸºäºå®é™…æŒ‡æ ‡çš„å†³ç­–ï¼Œè€Œä¸æ˜¯çŒœæµ‹
        recent_queries = [q for q in self.query_history[-100:] if q["timestamp"] > datetime.now().timestamp() - 3600]
        
        if len(recent_queries) < 10:
            return False
        
        # è®¡ç®—é”™è¯¯ç‡
        error_rate = len([q for q in recent_queries if not q["success"]]) / len(recent_queries)
        
        # æ£€æŸ¥æ€§èƒ½
        avg_time = sum(q["response_time"] for q in recent_queries if q["success"]) / len(recent_queries)
        
        # åˆ‡æ¢æ¡ä»¶ï¼ˆå¯é…ç½®ï¼‰
        switch_conditions = {
            "error_rate_too_high": error_rate > 0.1,  # é”™è¯¯ç‡è¶…è¿‡10%
            "response_too_slow": avg_time > 3.0,  # å¹³å‡å“åº”è¶…è¿‡3ç§’
            "framework_errors_high": self.metrics["framework_errors"] > 10  # æ¡†æ¶é”™è¯¯è¶…è¿‡10æ¬¡
        }
        
        return any(switch_conditions.values())
    
    def _try_fallback_framework(self, query: str) -> Dict[str, Any]:
        """å°è¯•åˆ‡æ¢åˆ°å¤‡ç”¨æ¡†æ¶"""
        print(f"âš ï¸ æ£€æµ‹åˆ°æ¡†æ¶é—®é¢˜ï¼Œå°è¯•åˆ‡æ¢åˆ°å¤‡ç”¨æ¡†æ¶...")
        
        # è¿™é‡Œå¯ä»¥å®ç°çƒ­åˆ‡æ¢é€»è¾‘
        # å½“å‰ç®€å•è¿”å›é”™è¯¯ä¿¡æ¯
        return {
            "answer": "ç³»ç»Ÿæ­£åœ¨ä¼˜åŒ–ï¼Œè¯·ç¨åå†è¯•",
            "sources": [],
            "response_time": 0,
            "framework": self.config.framework,
            "success": False,
            "fallback_triggered": True,
            "switch_advice": self._get_switch_advice()
        }
    
    def _get_switch_advice(self) -> Dict[str, Any]:
        """æä¾›æ¡†æ¶åˆ‡æ¢çš„å…·ä½“å»ºè®®"""
        current = self.config.framework
        target = "langchain" if current == "llamaindex" else "llamaindex"
        
        return {
            "current_framework": current,
            "recommended_framework": target,
            "estimated_effort": self.config.switch_cost_estimate.get(f"{current}_to_{target}", {}),
            "steps": [
                "1. æ›´æ–°é…ç½®ä¸­çš„frameworkå­—æ®µ",
                "2. é‡å¯æœåŠ¡ï¼ˆé€‚é…å™¨ä¼šè‡ªåŠ¨åˆ‡æ¢ï¼‰",
                "3. ç›‘æ§æ–°æ¡†æ¶çš„æ€§èƒ½æŒ‡æ ‡"
            ],
            "rollback_steps": [
                "1. æ¢å¤åŸé…ç½®",
                "2. é‡å¯æœåŠ¡"
            ]
        }
    
    def switch_framework(self, new_framework: str):
        """åŠ¨æ€åˆ‡æ¢æ¡†æ¶ï¼ˆæ¼”ç¤ºç”¨ï¼Œç”Ÿäº§ç¯å¢ƒéœ€è¦æ›´å¤æ‚çš„è¿ç§»ï¼‰"""
        print(f"ğŸ”„ åˆ‡æ¢æ¡†æ¶: {self.config.framework} -> {new_framework}")
        
        # è®°å½•åˆ‡æ¢å†³ç­–
        switch_log = {
            "timestamp": datetime.now(),
            "from": self.config.framework,
            "to": new_framework,
            "reason": self.metrics,
            "history_size": len(self.query_history)
        }
        
        # æ›´æ–°é…ç½®
        old_config = self.config
        self.config = RAGConfig(
            framework=new_framework,
            embedding_model=old_config.embedding_model,
            llm_model=old_config.llm_model
        )
        
        # åœ¨å®é™…ç³»ç»Ÿä¸­ï¼Œè¿™é‡Œéœ€è¦é‡æ–°åˆå§‹åŒ–ç»„ä»¶
        # ä½†å› ä¸ºæˆ‘ä»¬æœ‰æŠ½è±¡å±‚ï¼Œåªéœ€ç”¨æ–°æ¡†æ¶é‡æ–°åˆ›å»ºå³å¯
        if self.vector_store and hasattr(self.vector_store, '_documents'):
            documents = getattr(self.vector_store, '_documents', [])
            self.ingest(documents)
        
        return switch_log


# ==================== 5. ä½¿ç”¨ç¤ºä¾‹ ====================

def main():
    """æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨è¿™ä¸ªå¯æ¼”è¿›çš„RAGç³»ç»Ÿ"""
    
    # 1. å®šä¹‰é…ç½® - è¿™æ˜¯å”¯ä¸€çš„å†³ç­–ç‚¹
    config = RAGConfig(
        framework="llamaindex",  # ä»Šå¤©é€‰æ‹©LlamaIndex
        embedding_model="text-embedding-3-small",
        llm_model="gpt-4-turbo"
    )
    
    print(f"ğŸ¯ åˆå§‹é…ç½®: ä½¿ç”¨ {config.framework}")
    print(f"ğŸ“Š åˆ‡æ¢æˆæœ¬é¢„ä¼°: {json.dumps(config.switch_cost_estimate, indent=2, ensure_ascii=False)}")
    
    # 2. åˆ›å»ºRAGç³»ç»Ÿ
    rag = SimpleRAGSystem(config)
    
    # 3. åˆ›å»ºä¸€äº›ç¤ºä¾‹æ–‡æ¡£
    class SimpleDocument(Document):
        def __init__(self, content: str, source: str = ""):
            self._content = content
            self._metadata = {"source": source, "id": hash(content)}
        
        @property
        def content(self) -> str:
            return self._content
        
        @property
        def metadata(self) -> Dict[str, Any]:
            return self._metadata
    
    documents = [
        SimpleDocument("LangChainæ˜¯ä¸€ä¸ªç”¨äºå¼€å‘å¤§è¯­è¨€æ¨¡å‹åº”ç”¨çš„æ¡†æ¶", "doc1"),
        SimpleDocument("LlamaIndexæ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºRAGä»»åŠ¡ä¼˜åŒ–çš„æ¡†æ¶", "doc2"),
        SimpleDocument("å‘é‡æ•°æ®åº“ç”¨äºå­˜å‚¨å’Œæ£€ç´¢åµŒå…¥å‘é‡", "doc3")
    ]
    
    # 4. æ‘„å–æ–‡æ¡£
    rag.ingest(documents)
    
    # 5. æŸ¥è¯¢
    result = rag.query("ä»€ä¹ˆæ˜¯RAGæ¡†æ¶ï¼Ÿ")
    print(f"\nğŸ” æŸ¥è¯¢ç»“æœ:")
    print(f"   ç­”æ¡ˆ: {result['answer'][:100]}...")
    print(f"   å“åº”æ—¶é—´: {result['response_time']:.2f}ç§’")
    print(f"   ä½¿ç”¨æ¡†æ¶: {result['framework']}")
    
    # 6. æ¼”ç¤ºåˆ‡æ¢å†³ç­–
    print(f"\nğŸ“ˆ å½“å‰æŒ‡æ ‡:")
    print(f"   æ€»æŸ¥è¯¢æ•°: {rag.metrics['total_queries']}")
    print(f"   å¹³å‡å“åº”æ—¶é—´: {rag.metrics['avg_response_time']:.2f}ç§’")
    print(f"   æ¡†æ¶é”™è¯¯æ•°: {rag.metrics['framework_errors']}")
    
    # 7. å¦‚æœéœ€è¦åˆ‡æ¢ï¼Œè·å–å…·ä½“å»ºè®®
    if rag._should_switch_framework():
        advice = rag._get_switch_advice()
        print(f"\nğŸ”„ åˆ‡æ¢å»ºè®®:")
        print(f"   å½“å‰æ¡†æ¶: {advice['current_framework']}")
        print(f"   æ¨èæ¡†æ¶: {advice['recommended_framework']}")
        print(f"   é¢„ä¼°å·¥ä½œé‡: {advice['estimated_effort'].get('estimated_hours', 'æœªçŸ¥')} å°æ—¶")
    
    # 8. æ¶æ„å¸ˆçš„ä»·å€¼ä½“ç°
    print(f"\nğŸ—ï¸ æ¶æ„å¸ˆè®¾è®¡çš„ä»·å€¼:")
    print(f"   1. æŠ½è±¡å±‚ä¿æŠ¤: ä¸šåŠ¡é€»è¾‘ä¸ä¾èµ–å…·ä½“æ¡†æ¶")
    print(f"   2. å¯é€†å†³ç­–: åˆ‡æ¢æ¡†æ¶åªéœ€æ”¹é…ç½®ï¼Œä¸éœ€é‡å†™ä¸šåŠ¡ä»£ç ")
    print(f"   3. æ•°æ®é©±åŠ¨: åŸºäºå®é™…æŒ‡æ ‡å†³å®šæ˜¯å¦åˆ‡æ¢ï¼Œè€ŒéçŒœæµ‹")
    print(f"   4. æˆæœ¬é€æ˜: æ¯ä¸ªå†³ç­–çš„åˆ‡æ¢æˆæœ¬éƒ½æœ‰æ˜ç¡®é¢„ä¼°")


# ==================== 6. æ¶æ„å¸ˆçš„é¢å¤–å·¥ä½œ ====================

class RAGSystemMonitor:
    """ç›‘æ§ç³»ç»Ÿï¼Œæ”¶é›†æ•°æ®ä»¥æ”¯æŒæ¶æ„å†³ç­–"""
    
    def __init__(self, rag_system: SimpleRAGSystem):
        self.rag = rag_system
        self.decision_log = []
    
    def evaluate_framework_decision(self) -> Dict[str, Any]:
        """åŸºäºæ•°æ®è¯„ä¼°å½“å‰æ¡†æ¶é€‰æ‹©æ˜¯å¦æ­£ç¡®"""
        
        if len(self.rag.query_history) < 20:
            return {"status": "insufficient_data", "recommendation": "ç»§ç»­æ”¶é›†æ•°æ®"}
        
        # åˆ†ææ€§èƒ½æŒ‡æ ‡
        recent = self.rag.query_history[-20:]
        success_rate = len([q for q in recent if q["success"]]) / len(recent)
        avg_time = sum(q["response_time"] for q in recent if q["success"]) / len(recent)
        
        # ä¸SLOå¯¹æ¯”
        meets_slo = {
            "success_rate": success_rate >= 0.95,
            "response_time": avg_time <= 3.0,
            "framework_stability": self.rag.metrics["framework_errors"] < 5
        }
        
        recommendation = "ä¿æŒå½“å‰æ¡†æ¶"
        if not all(meets_slo.values()):
            recommendation = f"è€ƒè™‘åˆ‡æ¢åˆ°{self.rag._get_switch_advice()['recommended_framework']}"
        
        evaluation = {
            "timestamp": datetime.now(),
            "current_framework": self.rag.config.framework,
            "metrics": {
                "success_rate": success_rate,
                "avg_response_time": avg_time,
                "framework_errors": self.rag.metrics["framework_errors"]
            },
            "meets_slo": meets_slo,
            "recommendation": recommendation,
            "data_points": len(recent)
        }
        
        self.decision_log.append(evaluation)
        return evaluation


if __name__ == "__main__":
    main()
